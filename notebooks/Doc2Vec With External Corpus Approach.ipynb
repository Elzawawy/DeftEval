{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9AKBxXvn4GKr"
   },
   "source": [
    "# **Solving the Definition Extraction Problem**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "apKAmqWk4TTX"
   },
   "source": [
    "### **Approach 3: Using Doc2Vec model and Classifiers.**\n",
    "\n",
    "**Doc2Vec** is a Model that represents each Document as a Vector. The goal of Doc2Vec is to create a numeric representation of a document, regardless of its length. So, the input of texts per document can be various while the output is fixed-length vectors.\n",
    "Design of Doc2Vec is based on Word2Vec. But unlike words, documents do not come in logical structures such as words, so the another method has to be found. There are two implementations:\n",
    "\n",
    "1.   Paragraph Vector - Distributed Memory (PV-DM)\n",
    "2.   Paragraph Vector - Distributed Bag of Words (PV-DBOW)\n",
    "\n",
    "**PV-DM** is analogous to Word2Vec continous bag of word CBOW. But instead of using just words to predict the next word, add another feature vector, which is document-unique. So, when training the word vectors W, the document vector D is trained as well, and in the end of training, it holds a numeric representation of the document.\n",
    "\n",
    "![alt text](https://quantdare.com/wp-content/uploads/2019/08/06.png)\n",
    "\n",
    "\n",
    "**PV-DBOW** is analogous to Word2Vec skip gram. Instead of predicting next word, it use a document vector to classify entire words in the document.\n",
    "\n",
    "![alt text](https://quantdare.com/wp-content/uploads/2019/08/07.png)\n",
    "\n",
    "\n",
    "Not: it's recommend to use a combination of both algorithms to infer the vector representation of a document. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "iepXo9YYMjFD",
    "outputId": "0889aef6-619a-470a-89e4-6e97877f8117"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "eIVC3zRjkGsH",
    "outputId": "484be073-c3b8-4480-fc5f-e125fc31bb2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  drive/My Drive/wikipedia-movie-plots.zip\n",
      "  inflating: wiki_movie_plots_deduped.csv  \n"
     ]
    }
   ],
   "source": [
    "!unzip 'drive/My Drive/wikipedia-movie-plots.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "s-lpsjP7TWLY",
    "outputId": "34566267-78a6-44c1-f62e-bf4357d28831"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pandas as pd \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from data_loader import DeftCorpusLoader\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_B9mNkQLqSzv"
   },
   "source": [
    "### **Load Doc2Vec Model Trainning Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "szLsVqXxYhgz",
    "outputId": "3546c622-22d7-48fe-f801-5997a7d0ac7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus legnth:  34886\n"
     ]
    }
   ],
   "source": [
    "# Load amazon review reports of movies.\n",
    "with open('wiki_movie_plots_deduped.csv') as data:\n",
    "  corpus_list = pd.read_csv(data, sep=\",\", header = None)\n",
    "corpus_list = corpus_list[7].tolist()[1:]\n",
    "print(\"Corpus legnth: \", len(corpus_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9BrgBWwLmK-9"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "porter = PorterStemmer()\n",
    "qoutes_list = [\"``\", \"\\\"\\\"\", \"''\"]\n",
    "train_corpus = []\n",
    "for i, sentence in enumerate(corpus_list):\n",
    "  \n",
    "  # Lower all the letters in the sentence\n",
    "  tokens = word_tokenize(sentence.lower())\n",
    "  processed_tokens = []\n",
    "  for j, token in enumerate(tokens):\n",
    "    if not token.isdigit():\n",
    "      if token not in stop_words and len(token) > 1 and token not in qoutes_list:\n",
    "\n",
    "        # Convert each sentence from amazon reviews to list of words that doesn't include\n",
    "        # stop words or any special letters or digits\n",
    "        processed_tokens.append(porter.stem(token))\n",
    "  train_corpus.append(TaggedDocument(words=processed_tokens, tags=[str(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "CmOXCm911oUs",
    "outputId": "2ef6b969-18ec-491f-ae80-027cb73c4fe7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['bartend', 'work', 'saloon', 'serv', 'drink', 'custom', 'fill', 'stereotyp', 'irish', 'man', \"'s\", 'bucket', 'beer', 'carri', 'nation', 'follow', 'burst', 'insid', 'assault', 'irish', 'man', 'pull', 'hat', 'eye', 'dump', 'beer', 'head', 'group', 'begin', 'wreck', 'bar', 'smash', 'fixtur', 'mirror', 'break', 'cash', 'regist', 'bartend', 'spray', 'seltzer', 'water', 'nation', \"'s\", 'face', 'group', 'policemen', 'appear', 'order', 'everybodi', 'leav'], tags=['0']),\n",
       " TaggedDocument(words=['moon', 'paint', 'smile', 'face', 'hang', 'park', 'night', 'young', 'coupl', 'walk', 'past', 'fenc', 'learn', 'rail', 'look', 'moon', 'smile', 'embrac', 'moon', \"'s\", 'smile', 'get', 'bigger', 'sit', 'bench', 'tree', 'moon', \"'s\", 'view', 'block', 'caus', 'frown', 'last', 'scene', 'man', 'fan', 'woman', 'hat', 'moon', 'left', 'sky', 'perch', 'shoulder', 'see', 'everyth', 'better'], tags=['1']),\n",
       " TaggedDocument(words=['film', 'minut', 'long', 'compos', 'two', 'shot', 'first', 'girl', 'sit', 'base', 'altar', 'tomb', 'face', 'hidden', 'camera', 'center', 'altar', 'view', 'portal', 'display', 'portrait', 'three', 'u.s.', 'presidents—abraham', 'lincoln', 'jame', 'a.', 'garfield', 'william', 'mckinley—each', 'victim', 'assassin', 'second', 'shot', 'run', 'eight', 'second', 'long', 'assassin', 'kneel', 'feet', 'ladi', 'justic'], tags=['2']),\n",
       " TaggedDocument(words=['last', 'second', 'consist', 'two', 'shot', 'first', 'shot', 'set', 'wood', 'winter', 'actor', 'repres', 'vice-presid', 'theodor', 'roosevelt', 'enthusiast', 'hurri', 'hillsid', 'toward', 'tree', 'foreground', 'fall', 'right', 'cock', 'rifl', 'two', 'men', 'bear', 'sign', 'read', 'photograph', 'press', 'agent', 'respect', 'follow', 'shot', 'photograph', 'set', 'camera', 'teddi', 'aim', 'rifl', 'upward', 'tree', 'fell', 'appear', 'common', 'hous', 'cat', 'proce', 'stab', 'teddi', 'hold', 'prize', 'aloft', 'press', 'agent', 'take', 'note', 'second', 'shot', 'taken', 'slightli', 'differ', 'part', 'wood', 'path', 'teddi', 'ride', 'path', 'hors', 'toward', 'camera', 'left', 'shot', 'follow', 'close', 'press', 'agent', 'photograph', 'still', 'duti', 'hold', 'sign'], tags=['3']),\n",
       " TaggedDocument(words=['earliest', 'known', 'adapt', 'classic', 'fairytal', 'film', 'show', 'jack', 'trade', 'cow', 'bean', 'mother', 'forc', 'drop', 'front', 'yard', 'beig', 'forc', 'upstair', 'sleep', 'jack', 'visit', 'fairi', 'show', 'glimps', 'await', 'ascend', 'bean', 'stalk', 'version', 'jack', 'son', 'depos', 'king', 'jack', 'wake', 'find', 'beanstalk', 'grown', 'climb', 'top', 'enter', 'giant', \"'s\", 'home', 'giant', 'find', 'jack', 'narrowli', 'escap', 'giant', 'chase', 'jack', 'bean', 'stalk', 'jack', 'abl', 'cut', 'giant', 'get', 'safeti', 'fall', 'kill', 'jack', 'celebr', 'fairi', 'reveal', 'jack', 'may', 'return', 'home', 'princ'], tags=['4'])]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86tVNipeD_h_"
   },
   "source": [
    "### **Train Doc2Vec Model Based on Amazon Reviews.**\n",
    "First we will define the attributes of Doc2Vec model:\n",
    "\n",
    "\n",
    "*   **Vector Size:** Dimensionality of the documents feature vector.\n",
    "*   **Min Count:** Ignores all words with total frequency lower than this.\n",
    "*   **Epochs:** Number of iterations (epochs) over the corpus.\n",
    "*   **Workers:** Use these many worker threads to train the model (faster training with multicore machines).\n",
    "\n",
    "Second build the **Vocabulary** based on the training corpus (processed amazon reviews). Finally train the model on the training corpus.\n",
    "\n",
    "Note: the default used algorithm is PV-DM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nANm0U59m_Zq"
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec(vector_size=300, min_count=2, epochs=40, workers=8)\n",
    "model.build_vocab(train_corpus)\n",
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ziLFmHzqxsJ"
   },
   "source": [
    "### **Load DeftEval Trainning & Dev Data**\n",
    "\n",
    "Note: as the code is executed on google colab, the path of the data is rooted from the drive. So, the path of the data need to be change if the code will  be executed on the local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bocr33jBpbC8"
   },
   "outputs": [],
   "source": [
    "deft_loader = DeftCorpusLoader(\"drive/My Drive/DeftEval/deft_corpus/data\")\n",
    "trainframe, devframe = deft_loader.load_classification_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sTEacSqapfbt"
   },
   "outputs": [],
   "source": [
    "deft_loader.preprocess_data(devframe)\n",
    "deft_loader.clean_data(devframe)\n",
    "dev_vectors = []\n",
    "\n",
    "# Create test data vectors from Doc2Vec model\n",
    "for parsed_list in devframe[\"Parsed\"]:\n",
    "  dev_vectors.append(model.infer_vector(parsed_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4OwSvboSphlk"
   },
   "outputs": [],
   "source": [
    "deft_loader.preprocess_data(trainframe)\n",
    "deft_loader.clean_data(trainframe)\n",
    "train_vectors=[]\n",
    "\n",
    "# Create training data vectors from Doc2Vec model\n",
    "for parsed_list in trainframe[\"Parsed\"]:\n",
    "  train_vectors.append(model.infer_vector(parsed_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s3n2l4lkp5Tv"
   },
   "source": [
    "### **Apply Classifiers Algorithms**\n",
    "\n",
    "For each classifier test, **F1-score** and **Accuracy** are calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aIQ18FDi2ZXr"
   },
   "source": [
    "**1. Naive Bayes Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "t-dAIYO1pkVN",
    "outputId": "ae3b1912-8c23-49a2-c0b7-05a819769965"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.69      0.69       510\n",
      "           1       0.42      0.42      0.42       275\n",
      "\n",
      "    accuracy                           0.60       785\n",
      "   macro avg       0.56      0.56      0.56       785\n",
      "weighted avg       0.60      0.60      0.60       785\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "test_predict = gnb.fit(train_vectors, trainframe['HasDef']).predict(dev_vectors)\n",
    "print(metrics.classification_report(list(devframe[\"HasDef\"]), test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7uju07DT2qoj"
   },
   "source": [
    "**2. Decision Tree Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "HgZw5ZqwptFo",
    "outputId": "2f93310b-8931-45d7-9348-09d33e0278a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.72      0.70       510\n",
      "           1       0.42      0.38      0.40       275\n",
      "\n",
      "    accuracy                           0.60       785\n",
      "   macro avg       0.55      0.55      0.55       785\n",
      "weighted avg       0.59      0.60      0.60       785\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decision_tree = tree.DecisionTreeClassifier(class_weight=\"balanced\")\n",
    "test_predict = decision_tree.fit(train_vectors, trainframe['HasDef']).predict(dev_vectors)\n",
    "print(metrics.classification_report(list(devframe[\"HasDef\"]), test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rvYEQzgD2zsB"
   },
   "source": [
    "**3. Logistic Regression Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "UKkVAfR3pyd2",
    "outputId": "ab14ccf1-7d38-4dbf-da33-cff4f2cf6f28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.69      0.72       510\n",
      "           1       0.49      0.55      0.52       275\n",
      "\n",
      "    accuracy                           0.64       785\n",
      "   macro avg       0.61      0.62      0.62       785\n",
      "weighted avg       0.65      0.64      0.65       785\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predict = LogisticRegression(class_weight=\"balanced\", random_state=0).fit(train_vectors, trainframe['HasDef']).predict(dev_vectors)\n",
    "print(metrics.classification_report(list(devframe[\"HasDef\"]), test_predict))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Doc2Vec With External Corpus Approach.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
