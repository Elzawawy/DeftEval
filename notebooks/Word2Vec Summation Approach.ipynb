{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Summation Word2Vec Approach.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maIfQW9VmRJ4",
        "colab_type": "text"
      },
      "source": [
        "# **Solving the Definition Extraction Problem**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIpIW-xgmTbX",
        "colab_type": "text"
      },
      "source": [
        "### **Approach 5: Using Summation Word2Vec model and Classifiers.**\n",
        "\n",
        "**Word2Vec** is a more recent model that embeds words in a lower-dimensional vector space using a shallow neural network. The result is a set of word-vectors where vectors close together in vector space have similar meanings based on context, and word-vectors distant to each other have differing meanings. For example, strong and powerful would be close together and strong and Paris would be relatively far.\n",
        "\n",
        "![alt text](https://www.smartcat.io/media/1395/3d_transparent.png?width=500&height=198.90795631825273)\n",
        "\n",
        "With the Word2Vec model, we can calculate the vectors for each word in a document. But what if we want to calculate a vector for the entire document?. We could use Word2Vec for this task by inferring a vector for each word in the document using Word2Vec model then summing all these words vectors to create one vector that represent the whole document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hchJE8OqepZ",
        "colab_type": "code",
        "outputId": "c84c9e25-ac0c-4082-f59b-c7310fbeb58c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Run this cell only if you are working on google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RIqQdFut_yS",
        "colab_type": "code",
        "outputId": "9c558592-c8c3-4a37-db86-baeab3287be0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "# Download GoogleNews word embeddings file\n",
        "!wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-17 19:58:14--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.186.237\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.186.237|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  90.4MB/s    in 17s     \n",
            "\n",
            "2020-01-17 19:58:31 (92.9 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbHnpgQCvE5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gunzip 'GoogleNews-vectors-negative300.bin.gz'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YL21EHRqWev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from data_loader import DeftCorpusLoader\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import tree\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaikbSPNxtbu",
        "colab_type": "text"
      },
      "source": [
        "### **Build Word2Vec Model Using GoogleNews Word Embeddings File**\n",
        "\n",
        "GoogleNew word embeddings file is a file contains vector representation for 3 millions word from google news. Eash word vector is 300 dimensions. We will load this file into genism Word2Vec model to vectorize document words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNnAbj5Fq3Nr",
        "colab_type": "code",
        "outputId": "589851e0-fbca-4aed-d42c-176f3426c51e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        " model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Zqvx57Wcg3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_documents_vectors(parsed_documents):\n",
        "  \"\"\"\n",
        "  Used to get the vector representation of a parsed document (preprocessed, cleaned & must be tokenized)\n",
        "\n",
        "  Args:\n",
        "    parsed_documents: List of tokenized docuemnts.\n",
        "\n",
        "  Returns:\n",
        "    vectors: List of vector representation of each document in parsed documents.\n",
        "  \"\"\"\n",
        "  vectors = []\n",
        "  for parsed_document in parsed_documents:\n",
        "\n",
        "    # Initialze a temp vector with size 300 for document vector\n",
        "    temp_vector = np.array([0] * 300)\n",
        "    for token in parsed_document:\n",
        "      if(token in model.vocab.keys()):\n",
        "\n",
        "        # Add the vector of the token to the temp vector\n",
        "        temp_vector = np.add(temp_vector, model.get_vector(token))\n",
        "    vectors.append(temp_vector)\n",
        "  return vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHwCwdAE1zkE",
        "colab_type": "text"
      },
      "source": [
        "### **Load DeftEval Trainning & Dev Data**\n",
        "\n",
        "Note: as the code is executed on google colab, the path of the data is rooted from the drive. So, the path of the data need to be change if the code will  be executed on the local machine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVUedg7dpwFS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "deft_loader = DeftCorpusLoader(\"drive/My Drive/DeftEval/deft_corpus/data\")\n",
        "trainframe, devframe = deft_loader.load_classification_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zx0w9CeJ12zG",
        "colab_type": "text"
      },
      "source": [
        "Preprocess training and dev data (remove stop words, stemming & tokenizing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIrGDAsKqbtg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "deft_loader.preprocess_data(devframe)\n",
        "deft_loader.clean_data(devframe)\n",
        "\n",
        "deft_loader.preprocess_data(trainframe)\n",
        "deft_loader.clean_data(trainframe)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdfLlNRd2EjJ",
        "colab_type": "text"
      },
      "source": [
        "Get the vector representation of each document in the training and dev data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQ-JGwn6dHYB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_vectors = get_documents_vectors(trainframe['Parsed'])\n",
        "dev_vectors = get_documents_vectors(devframe['Parsed'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkwc6BH82QHh",
        "colab_type": "text"
      },
      "source": [
        "### **Apply Classifiers Algorithms**\n",
        "\n",
        "For each classifier test, **F1-score** and **Accuracy** are calculated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYED2XU62TNa",
        "colab_type": "text"
      },
      "source": [
        "**1. Naive Bayes Algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qeDsypIx-PV",
        "colab_type": "code",
        "outputId": "6be434c5-578a-446a-b78a-b4344f297d70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "gnb = GaussianNB()\n",
        "test_predict = gnb.fit(train_vectors, trainframe['HasDef']).predict(dev_vectors)\n",
        "print(metrics.classification_report(list(devframe[\"HasDef\"]), test_predict))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.79      0.74       510\n",
            "           1       0.48      0.35      0.41       275\n",
            "\n",
            "    accuracy                           0.64       785\n",
            "   macro avg       0.59      0.57      0.57       785\n",
            "weighted avg       0.62      0.64      0.62       785\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4vyhArV2dIx",
        "colab_type": "text"
      },
      "source": [
        "**2. Decision Tree**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-180bkbWH3NU",
        "colab_type": "code",
        "outputId": "17a876af-2645-4815-903b-95e1b6c84e9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "decision_tree = tree.DecisionTreeClassifier(class_weight=\"balanced\")\n",
        "test_predict = decision_tree.fit(train_vectors, trainframe['HasDef']).predict(dev_vectors)\n",
        "print(metrics.classification_report(list(devframe[\"HasDef\"]), test_predict))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.73      0.74       510\n",
            "           1       0.52      0.53      0.52       275\n",
            "\n",
            "    accuracy                           0.66       785\n",
            "   macro avg       0.63      0.63      0.63       785\n",
            "weighted avg       0.66      0.66      0.66       785\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAnScAal2jdz",
        "colab_type": "text"
      },
      "source": [
        "**3. Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRt1drQG_fn_",
        "colab_type": "code",
        "outputId": "1e241089-3fd8-4083-f4b1-37dabce5035b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "test_predict = LogisticRegression(class_weight=\"balanced\", random_state=0).fit(train_vectors, trainframe['HasDef']).predict(dev_vectors)\n",
        "print(metrics.classification_report(list(devframe[\"HasDef\"]), test_predict))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.74      0.76       510\n",
            "           1       0.56      0.60      0.58       275\n",
            "\n",
            "    accuracy                           0.69       785\n",
            "   macro avg       0.66      0.67      0.67       785\n",
            "weighted avg       0.70      0.69      0.69       785\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dqFBC72xaIj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}